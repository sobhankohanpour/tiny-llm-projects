{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d2117a7-062e-4666-ba47-fad0cb6ff733",
   "metadata": {},
   "source": [
    "# Ask Your PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab23524d",
   "metadata": {},
   "source": [
    "## Import LlamaIndex and Ollama Modules\n",
    "\n",
    "This cell imports the necessary components to:\n",
    "\n",
    "- Read and process documents (SimpleDirectoryReader)\n",
    "- Build and configure a vector index (VectorStoreIndex, Settings)\n",
    "- Use Ollama for embeddings and language model processing (Ollama, OllamaEmbedding)\n",
    "- Save and load indexes for future use (StorageContext, load_index_from_storage)\n",
    "\n",
    "With these imports, you’re ready to create a searchable index from your documents and interact with it using Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2982fa7-6146-40ea-9d99-218777e1e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "from llama_index.core import StorageContext, load_index_from_storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4209512",
   "metadata": {},
   "source": [
    "## Configuring LLM, Chunking, and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d3004-dcbd-4bff-9745-7d1de62e596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM (for answering)\n",
    "Settings.llm = Ollama(\n",
    "    model=\"qwen3:0.6b\",   # smaller and faster that \"qwen3:1.7b\"\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "# Chunking settings (for faster embeddings)\n",
    "Settings.chunk_size = 1024      # Default: ~512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "# Embeddings (for indexing / retrieval)\n",
    "Settings.embed_model = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279ad2f4",
   "metadata": {},
   "source": [
    "## Loading Documents from a Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021a1ee7-df19-4286-b984-f8c0109ee869",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed834df",
   "metadata": {},
   "source": [
    "## Loading Documents and Building/Using the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fbfb97",
   "metadata": {},
   "source": [
    "The code below is currently using the files stored from the first run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2722c-4800-4699-bae7-91a1a6c7c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f4178",
   "metadata": {},
   "source": [
    "If you want to rebuild the index and adjust the accuracy according to your goal, you should change the cell type from raw to Python and run the following code instead of loading from storage:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b69c898-432d-41bf-a722-7a9c4aed0404",
   "metadata": {},
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    show_progress=True,\n",
    "    insert_batch_size=4 # Safe for 8 GB RAM\n",
    ")\n",
    "\n",
    "index.storage_context.persist(\"./storage\")\n",
    "print(\"✅ Index built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6d782",
   "metadata": {},
   "source": [
    "## Querying the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973bf1cd-e71f-4d4f-b141-40649d8bc9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is this document about?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
